{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd32f2c9",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f43ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "NumPy version: 2.3.5\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa0dd7",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374eee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(input_file='preprocessed_data.pkl'):\n",
    "\n",
    "    print(f\"Loading preprocessed data from {input_file}...\")\n",
    "    \n",
    "    with open(input_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(\"Preprocessed data loaded successfully!\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def print_data_statistics(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print(\"\\nDATASET STATISTICS\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Total Samples: {len(X_train) + len(X_test)}\")\n",
    "    print(f\"Training Samples: {len(X_train)}\")\n",
    "    print(f\"Test Samples: {len(X_test)}\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    print(f\"Number of Classes: {len(np.unique(y_train))}\")\n",
    "    print(f\"\\nTraining Set Distribution: {np.bincount(y_train)}\")\n",
    "    print(f\"Test Set Distribution: {np.bincount(y_test)}\")\n",
    "    print(f\"\\nPixel Value Mean: {np.mean(X_train):.4f}\")\n",
    "    print(f\"Pixel Value Std: {np.std(X_train):.4f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9bad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from preprocessed_data.pkl...\n",
      "Preprocessed data loaded successfully!\n",
      "Training set: 8000 samples\n",
      "Testing set: 2000 samples\n",
      "\n",
      "DATASET STATISTICS\n",
      "----------------------------------------\n",
      "Total Samples: 10000\n",
      "Training Samples: 8000\n",
      "Test Samples: 2000\n",
      "Features: 784\n",
      "Number of Classes: 10\n",
      "\n",
      "Training Set Distribution: [800 800 800 800 800 800 800 800 800 800]\n",
      "Test Set Distribution: [200 200 200 200 200 200 200 200 200 200]\n",
      "\n",
      "Pixel Value Mean: 0.4265\n",
      "Pixel Value Std: 0.1665\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_preprocessed_data()\n",
    "print_data_statistics(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27943bc4",
   "metadata": {},
   "source": [
    "## 3. Linear Regression (One-vs-All) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64bbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionBGD:\n",
    "\n",
    "    def __init__(self, alpha=0.01, n_iters=1000, random_state=42):\n",
    "        self.alpha = alpha\n",
    "        self.n_iters = n_iters\n",
    "        self.random_state = random_state\n",
    "        self.theta = None\n",
    "\n",
    "    \n",
    "    def _add_bias(self, X):\n",
    "        return np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "    def _compute_cost(self, X, y, theta):\n",
    "        m = len(y)\n",
    "        predictions = X @ theta\n",
    "        cost = (1/m) * np.sum((predictions - y)**2)\n",
    "        return cost\n",
    "    \n",
    "    def _bgd(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)\n",
    "        old_cost = 1e9\n",
    "        count = 0\n",
    "        for i in range(self.n_iters):\n",
    "            predictions = X @ self.theta\n",
    "            error = predictions - y\n",
    "            grad = (2/m) * (X.T @ error)\n",
    "            self.theta -= self.alpha * grad\n",
    "            if ( i%100 == 0 ):\n",
    "                new_cost = self._compute_cost(X,y,self.theta)\n",
    "                print(f\"Cost\\t:\\t{new_cost}\") \n",
    "                if (new_cost > old_cost):\n",
    "                    count +=1\n",
    "                    if count == 5 :\n",
    "                        break\n",
    "                old_cost = new_cost \n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_b = self._add_bias(X)\n",
    "        self.theta = self._bgd(X_b, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        X_b = self._add_bias(X)\n",
    "        return X_b @ self.theta\n",
    "\n",
    "\n",
    "def train_one_vs_all_linear_regression(X_train, y_train, alpha=0.01, n_iters=1000, verbose=True):\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING LINEAR REGRESSION (ONE-VS-ALL)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Learning rate (alpha): {alpha}\")\n",
    "        print(f\"Iterations: {n_iters}\")\n",
    "        print(\"-\"*60)\n",
    "    \n",
    "    classes = np.unique(y_train)\n",
    "    models = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for cls in classes:\n",
    "        if verbose:\n",
    "            print(f\"Training classifier for class {cls}...\")\n",
    "        \n",
    "        y_binary = (y_train == cls).astype(float)\n",
    "        \n",
    "        model = LinearRegressionBGD(alpha=alpha, n_iters=n_iters, random_state=42)\n",
    "        model.fit(X_train, y_binary)\n",
    "        \n",
    "        models[cls] = model\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return models, training_time\n",
    "\n",
    "\n",
    "def predict_linear_regression(models, X):\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    n_classes = len(models)\n",
    "    \n",
    "\n",
    "    continuous_predictions = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for cls, model in models.items():\n",
    "        continuous_predictions[:, cls] = model.predict(X)\n",
    "    \n",
    "    y_pred = np.argmax(continuous_predictions, axis=1)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d06e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING LINEAR REGRESSION (ONE-VS-ALL)\n",
      "============================================================\n",
      "Learning rate (alpha): 0.0055\n",
      "Iterations: 1000\n",
      "------------------------------------------------------------\n",
      "Training classifier for class 0...\n",
      "Cost\t:\t0.09370051701806588\n",
      "Cost\t:\t0.0663244753410007\n",
      "Cost\t:\t0.059513742263717355\n",
      "Cost\t:\t0.05658776532838408\n",
      "Cost\t:\t0.05502476565145476\n",
      "Cost\t:\t0.054090555589155986\n",
      "Cost\t:\t0.05349084545264712\n",
      "Cost\t:\t0.053082789904571374\n",
      "Cost\t:\t0.052790285383783414\n",
      "Cost\t:\t0.052570628092225534\n",
      "Training classifier for class 1...\n",
      "Cost\t:\t0.09477709200656395\n",
      "Cost\t:\t0.07202848531995386\n",
      "Cost\t:\t0.06682740660643509\n",
      "Cost\t:\t0.06480564563851925\n",
      "Cost\t:\t0.0637636022273404\n",
      "Cost\t:\t0.06312346131356039\n",
      "Cost\t:\t0.0626870244938394\n",
      "Cost\t:\t0.06236850093123314\n",
      "Cost\t:\t0.06212414436455294\n",
      "Cost\t:\t0.061929166426882634\n",
      "Training classifier for class 2...\n",
      "Cost\t:\t0.09465305234123923\n",
      "Cost\t:\t0.07989902175202246\n",
      "Cost\t:\t0.07366517770240211\n",
      "Cost\t:\t0.06967669960825938\n",
      "Cost\t:\t0.06699118399645397\n",
      "Cost\t:\t0.06511770592599289\n",
      "Cost\t:\t0.06377292041940902\n",
      "Cost\t:\t0.06278321201659873\n",
      "Cost\t:\t0.062037999815180725\n",
      "Cost\t:\t0.061464819235365974\n",
      "Training classifier for class 3...\n",
      "Cost\t:\t0.09408032750027998\n",
      "Cost\t:\t0.07316069175960928\n",
      "Cost\t:\t0.06571032239956114\n",
      "Cost\t:\t0.061834309346161374\n",
      "Cost\t:\t0.05955997107550419\n",
      "Cost\t:\t0.05809257007066549\n",
      "Cost\t:\t0.05707681208871954\n",
      "Cost\t:\t0.056336354840609726\n",
      "Cost\t:\t0.055774972846037336\n",
      "Cost\t:\t0.05533592124879951\n",
      "Training classifier for class 4...\n",
      "Cost\t:\t0.09447085995013853\n",
      "Cost\t:\t0.07499331855833051\n",
      "Cost\t:\t0.06824374762347027\n",
      "Cost\t:\t0.06473276643619752\n",
      "Cost\t:\t0.0626802850997167\n",
      "Cost\t:\t0.06137558599997395\n",
      "Cost\t:\t0.06049127511837064\n",
      "Cost\t:\t0.05986018603864229\n",
      "Cost\t:\t0.05939043232777984\n",
      "Cost\t:\t0.05902851480857002\n",
      "Training classifier for class 5...\n",
      "Cost\t:\t0.09442889573635956\n",
      "Cost\t:\t0.07762531803830242\n",
      "Cost\t:\t0.07227192924075466\n",
      "Cost\t:\t0.06954108383818784\n",
      "Cost\t:\t0.06790693563548138\n",
      "Cost\t:\t0.06680605745833128\n",
      "Cost\t:\t0.06600367691850761\n",
      "Cost\t:\t0.06538804611576304\n",
      "Cost\t:\t0.06489879127072527\n",
      "Cost\t:\t0.06449974010244458\n",
      "Training classifier for class 6...\n",
      "Cost\t:\t0.09415170601751019\n",
      "Cost\t:\t0.07081514625230939\n",
      "Cost\t:\t0.06351104453571142\n",
      "Cost\t:\t0.05990934174612346\n",
      "Cost\t:\t0.05782200910059604\n",
      "Cost\t:\t0.05648827528811681\n",
      "Cost\t:\t0.05558122056111465\n",
      "Cost\t:\t0.054936121576076656\n",
      "Cost\t:\t0.05446053792287672\n",
      "Cost\t:\t0.05409880032937625\n",
      "Training classifier for class 7...\n",
      "Cost\t:\t0.09471124673431534\n",
      "Cost\t:\t0.07875565893537763\n",
      "Cost\t:\t0.0743754202018514\n",
      "Cost\t:\t0.07193708201723081\n",
      "Cost\t:\t0.07027607194051669\n",
      "Cost\t:\t0.06905034209328165\n",
      "Cost\t:\t0.06811153605087109\n",
      "Cost\t:\t0.06737607901228651\n",
      "Cost\t:\t0.0667901927111301\n",
      "Cost\t:\t0.06631685265729285\n",
      "Training classifier for class 8...\n",
      "Cost\t:\t0.09410843296417551\n",
      "Cost\t:\t0.08014323263940543\n",
      "Cost\t:\t0.0757536413740124\n",
      "Cost\t:\t0.0734076264849555\n",
      "Cost\t:\t0.07200302936973779\n",
      "Cost\t:\t0.0710854767801787\n",
      "Cost\t:\t0.07044539804222888\n",
      "Cost\t:\t0.06997580611627435\n",
      "Cost\t:\t0.06961731993441234\n",
      "Cost\t:\t0.06933473052209271\n",
      "Training classifier for class 9...\n",
      "Cost\t:\t0.09465998139318776\n",
      "Cost\t:\t0.07753172894130983\n",
      "Cost\t:\t0.07178831117721035\n",
      "Cost\t:\t0.06874626870570634\n",
      "Cost\t:\t0.0669106853914088\n",
      "Cost\t:\t0.0657010060910949\n",
      "Cost\t:\t0.06485272348850897\n",
      "Cost\t:\t0.06422951282605364\n",
      "Cost\t:\t0.0637544616834518\n",
      "Cost\t:\t0.06338117928150365\n",
      "\n",
      "Training completed in 98.18 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "models, training_time = train_one_vs_all_linear_regression(\n",
    "    X_train, y_train,\n",
    "    alpha=0.0055,\n",
    "    n_iters=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc539c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_linear_regression(models, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd163a",
   "metadata": {},
   "source": [
    "### Evaluation Metrics (Accuracy, Precision, Recall, F1 , Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccf1b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PERFORMANCE\n",
      "====================\n",
      "Accuracy : 72.35%\n",
      "Precision: 72.5494%\n",
      "Recall   : 72.3500%\n",
      "F1-score : 72.2062%\n",
      "\n",
      "Confusion Matrix:\n",
      " [[153   6   2   5  10   6  10   3   0   5]\n",
      " [  7 158   0   1  13   6   3   4   7   1]\n",
      " [  3   3 146   8   5   6  10   9   3   7]\n",
      " [  3   6  10 165   4   2   0   2   4   4]\n",
      " [  2  23   2   3 140   4   9   2  10   5]\n",
      " [ 19   4   1   5   5 131  17   9   5   4]\n",
      " [  4   7   4   1   8  11 162   0   3   0]\n",
      " [  3  14   5   1   4   3   1 136   7  26]\n",
      " [  5   7   3  14  21  12  10   8 110  10]\n",
      " [  5   2   7   6  10   2   1  19   2 146]]\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "accuracy  = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall    = recall_score(y_test, y_pred, average='macro')\n",
    "f1        = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"====================\")\n",
    "print(f\"Accuracy : {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.4f}%\")\n",
    "print(f\"Recall   : {recall*100:.4f}%\")\n",
    "print(f\"F1-score : {f1*100:.4f}%\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def067b0",
   "metadata": {},
   "source": [
    "## 4 . Linear Regression Truning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35499248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_linear_regression_alpha(X_train, y_train, X_val, y_val):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LINEAR REGRESSION HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    alphas = [0.001, 0.005, 0.0055, 0.01, 0.1]\n",
    "    best_score = 0\n",
    "    best_alpha = 1.0\n",
    "    results = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"\\nTesting alpha={alpha}\")\n",
    "        models, train_time = train_one_vs_all_linear_regression(\n",
    "            X_train, y_train, alpha=alpha, n_iters=30, verbose=False\n",
    "        )\n",
    "    \n",
    "        y_pred_val = predict_linear_regression(models, X_val)\n",
    "        y_pred_train = predict_linear_regression(models, X_train)\n",
    "    \n",
    "        val_accuracy = np.mean(y_pred_val == y_val)\n",
    "        train_accuracy = np.mean(y_pred_train == y_train)\n",
    "    \n",
    "        result = {\n",
    "            'alpha': alpha,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': train_time\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "        print(f\"\\tTrain: {train_accuracy:.4f}, Val: {val_accuracy:.4f}, Time: {train_time:.2f}s\")\n",
    "    \n",
    "        if val_accuracy > best_score:\n",
    "            best_score = val_accuracy\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEST ALPHA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Alpha: {best_alpha}\")\n",
    "    print(f\"Validation Accuracy: {best_score:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return best_alpha,  results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ec0fe",
   "metadata": {},
   "source": [
    "## 4.1. Linear Regression Truning\n",
    "###     Create Validation Set for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28b1ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set for tuning: 6400 samples\n",
      "Validation set for tuning: 1600 samples\n"
     ]
    }
   ],
   "source": [
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set for tuning: {X_train_tune.shape[0]} samples\")\n",
    "print(f\"Validation set for tuning: {X_val_tune.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755d330",
   "metadata": {},
   "source": [
    "## 4.2. Linear Regression Truning\n",
    "\n",
    "**Parameter to test:**\n",
    "- **Alpha**: Learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c8d37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LINEAR REGRESSION HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "\n",
      "Testing alpha=0.005\n",
      "Cost\t:\t0.11580866463789818\n",
      "Cost\t:\t0.11831960706867392\n",
      "Cost\t:\t0.11759277536804337\n",
      "Cost\t:\t0.11625586858398337\n",
      "Cost\t:\t0.11731107555773708\n",
      "Cost\t:\t0.11707781431231054\n",
      "Cost\t:\t0.11642887494325482\n",
      "Cost\t:\t0.11771246595048811\n",
      "Cost\t:\t0.11658407807551832\n",
      "Cost\t:\t0.11753950912587559\n",
      "\tTrain: 0.1000, Val: 0.1000, Time: 2.55s\n",
      "\n",
      "Testing alpha=0.0055\n",
      "Cost\t:\t0.12455448943344212\n",
      "Cost\t:\t0.128380559605793\n",
      "Cost\t:\t0.12718056715502463\n",
      "Cost\t:\t0.12516597137087465\n",
      "Cost\t:\t0.12677941866713752\n",
      "Cost\t:\t0.1264077805422388\n",
      "Cost\t:\t0.1254591868840837\n",
      "Cost\t:\t0.1273943768563181\n",
      "Cost\t:\t0.12563112675713353\n",
      "Cost\t:\t0.1271240829398073\n",
      "\tTrain: 0.1000, Val: 0.1000, Time: 2.83s\n",
      "\n",
      "============================================================\n",
      "BEST ALPHA\n",
      "============================================================\n",
      "Alpha: 0.005\n",
      "Validation Accuracy: 0.1000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "linear_best_alpha, linear_tuning_results = tune_linear_regression_alpha(\n",
    "    X_train_tune, y_train_tune, X_val_tune, y_val_tune\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca807c",
   "metadata": {},
   "source": [
    "## 5 . Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae2fd0",
   "metadata": {},
   "source": [
    "### 5.1 . CROSS-VALIDATION (k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(X, y, alpha=0.005, n_iters=1000, k=5):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"K-FOLD CROSS-VALIDATION (k=5)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)  \n",
    "    cv_scores = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        print(f\"Fold {fold}/{k}... \\n\", end='')\n",
    "        \n",
    "        X_train_fold = X[train_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        X_val_fold = X[val_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        models, _ = train_one_vs_all_linear_regression(\n",
    "            X_train_fold, y_train_fold, \n",
    "            alpha=alpha, n_iters=n_iters, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = predict_linear_regression(models, X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_val_fold, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        cv_scores['accuracy'].append(accuracy)\n",
    "        cv_scores['precision'].append(precision)\n",
    "        cv_scores['recall'].append(recall)\n",
    "        cv_scores['f1'].append(f1)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        fold += 1\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:  {np.mean(cv_scores['accuracy']):.4f} ± {np.std(cv_scores['accuracy']):.4f}\")\n",
    "    print(f\"Precision: {np.mean(cv_scores['precision']):.4f} ± {np.std(cv_scores['precision']):.4f}\")\n",
    "    print(f\"Recall:    {np.mean(cv_scores['recall']):.4f} ± {np.std(cv_scores['recall']):.4f}\")\n",
    "    print(f\"F1-Score:  {np.mean(cv_scores['f1']):.4f} ± {np.std(cv_scores['f1']):.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return cv_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdf77c",
   "metadata": {},
   "source": [
    "### 5.2 . COMPREHENSIVE METRICS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6292624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_metrics(y_test, y_pred):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Per-class report\n",
    "    print(f\"\\nPer-Class Performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ccffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "K-FOLD CROSS-VALIDATION (k=5)\n",
      "============================================================\n",
      "Fold 1/5... \n",
      "Cost\t:\t0.09197371988013903\n",
      "Cost\t:\t0.09498329823925492\n",
      "Cost\t:\t0.09118665777898045\n",
      "Cost\t:\t0.09150396118333694\n",
      "Cost\t:\t0.09315716204683754\n",
      "Cost\t:\t0.0921772240023099\n",
      "Cost\t:\t0.09375840429131575\n",
      "Cost\t:\t0.09306289002847581\n",
      "Cost\t:\t0.09386977794232367\n",
      "Cost\t:\t0.09058506952713029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run this in your notebook after training your model:\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Cross-Validation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m cv_scores = \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mperform_cross_validation\u001b[39m\u001b[34m(X, y, alpha, n_iters, k)\u001b[39m\n\u001b[32m     22\u001b[39m y_val_fold = y[val_idx]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m models, _ = \u001b[43mtrain_one_vs_all_linear_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     31\u001b[39m y_pred = predict_linear_regression(models, X_val_fold)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mtrain_one_vs_all_linear_regression\u001b[39m\u001b[34m(X_train, y_train, alpha, n_iters, verbose)\u001b[39m\n\u001b[32m     73\u001b[39m     y_binary = (y_train == \u001b[38;5;28mcls\u001b[39m).astype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     75\u001b[39m     model = LinearRegressionBGD(alpha=alpha, n_iters=n_iters, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     models[\u001b[38;5;28mcls\u001b[39m] = model\n\u001b[32m     80\u001b[39m training_time = time.time() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mLinearRegressionBGD.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[32m     41\u001b[39m     X_b = \u001b[38;5;28mself\u001b[39m._add_bias(X)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28mself\u001b[39m.theta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mLinearRegressionBGD._bgd\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     27\u001b[39m grad = (\u001b[32m2\u001b[39m/m) * (X.T @ error)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.theta -= \u001b[38;5;28mself\u001b[39m.alpha * grad\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ( i%\u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m ):\n\u001b[32m     30\u001b[39m     new_cost = \u001b[38;5;28mself\u001b[39m._compute_cost(X,y,\u001b[38;5;28mself\u001b[39m.theta)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCost\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnew_cost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run this in your notebook after training your model:\n",
    "\n",
    "# 1. Cross-Validation\n",
    "cv_scores = perform_cross_validation(X_train, y_train, alpha=0.005, n_iters=100)\n",
    "\n",
    "# 2. Show all metrics\n",
    "show_all_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
